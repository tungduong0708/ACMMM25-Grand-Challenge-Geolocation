{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59988321-d66c-4d1b-904a-b640785410e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:54:16.936805Z",
     "iopub.status.busy": "2025-05-29T11:54:16.936460Z",
     "iopub.status.idle": "2025-05-29T11:54:27.759098Z",
     "shell.execute_reply": "2025-05-29T11:54:27.758057Z",
     "shell.execute_reply.started": "2025-05-29T11:54:16.936763Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'G3-Original'...\n",
      "remote: Enumerating objects: 409, done.\u001b[K\n",
      "remote: Counting objects: 100% (171/171), done.\u001b[K\n",
      "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
      "remote: Total 409 (delta 96), reused 115 (delta 44), pack-reused 238 (from 1)\u001b[K\n",
      "Receiving objects: 100% (409/409), 37.80 MiB | 27.57 MiB/s, done.\n",
      "Resolving deltas: 100% (250/250), done.\n",
      "/kaggle/working/G3-Original\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tungduong0708/G3-Original.git\n",
    "%cd G3-Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac821eed-2156-4014-9784-f2d8d467eb7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:54:27.760372Z",
     "iopub.status.busy": "2025-05-29T11:54:27.760067Z",
     "iopub.status.idle": "2025-05-29T11:54:27.766615Z",
     "shell.execute_reply": "2025-05-29T11:54:27.765716Z",
     "shell.execute_reply.started": "2025-05-29T11:54:27.760335Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/G3-Original'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f13be68",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-29T11:54:27.768074Z",
     "iopub.status.busy": "2025-05-29T11:54:27.767455Z",
     "iopub.status.idle": "2025-05-29T11:57:31.218366Z",
     "shell.execute_reply": "2025-05-29T11:57:31.217246Z",
     "shell.execute_reply.started": "2025-05-29T11:54:27.768053Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch==2.4.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.4.0%2Bcu124-cp311-cp311-linux_x86_64.whl (797.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.19.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.19.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.4.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.4.0%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.99 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (24.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.99 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.4/883.4 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.99 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.2.65 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.2.65-py3-none-manylinux2014_x86_64.whl (363.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.0.44 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.0.44-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.119 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.119-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.0.99 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.0.99-py3-none-manylinux2014_x86_64.whl (128.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.0.142 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.0.142-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.99 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m54.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.99 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.19.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.19.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.19.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.19.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.19.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.19.0) (2.4.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.19.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.19.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.19.0) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.19.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.19.0) (2024.2.0)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.6.0+cu124\n",
      "    Uninstalling torchaudio-2.6.0+cu124:\n",
      "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cu124\n",
      "    Uninstalling torchvision-0.21.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\n",
      "Successfully installed nvidia-cublas-cu12-12.4.2.65 nvidia-cuda-cupti-cu12-12.4.99 nvidia-cuda-nvrtc-cu12-12.4.99 nvidia-cuda-runtime-cu12-12.4.99 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.0.44 nvidia-curand-cu12-10.3.5.119 nvidia-cusolver-cu12-11.6.0.99 nvidia-cusparse-cu12-12.3.0.142 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.4.99 torch-2.4.0+cu124 torchaudio-2.4.0+cu124 torchvision-0.19.0+cu124 triton-3.0.0\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
      "Requirement already satisfied: pyproj in /usr/local/lib/python3.11/dist-packages (3.7.1)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
      "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
      "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
      "Requirement already satisfied: cartopy in /usr/local/lib/python3.11/dist-packages (0.24.1)\n",
      "Collecting faiss-gpu-cu12\n",
      "  Downloading faiss_gpu_cu12-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandarallel\n",
      "  Downloading pandarallel-1.6.5.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting retrying\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.4.0+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj) (2025.4.26)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy) (2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.20.3)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: fiona>=1.8.21 in /usr/local/lib/python3.11/dist-packages (from geopandas) (1.10.1)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.1.0)\n",
      "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.4.2.65)\n",
      "Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from pandarallel) (0.3.8)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.21->geopandas) (25.3.0)\n",
      "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.21->geopandas) (8.1.8)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.21->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.21->geopandas) (0.7.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading faiss_gpu_cu12-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: pandarallel\n",
      "  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pandarallel: filename=pandarallel-1.6.5-py3-none-any.whl size=16674 sha256=878c3ced70c8433c10f0e365e8018d62f95b752955ff7b4a73ec7f180eabc7de\n",
      "  Stored in directory: /root/.cache/pip/wheels/b9/c6/5a/829298789e94348b81af52ab42c19d49da007306bbcc983827\n",
      "Successfully built pandarallel\n",
      "Installing collected packages: retrying, pandarallel, faiss-gpu-cu12\n",
      "Successfully installed faiss-gpu-cu12-1.11.0 pandarallel-1.6.5 retrying-1.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install transformers accelerate huggingface_hub pandas optuna pyproj einops geopy matplotlib kaggle  geopandas cartopy faiss-gpu-cu12 pandarallel retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1acad77-8612-43fd-a8e6-31af4daf8bab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:57:31.221724Z",
     "iopub.status.busy": "2025-05-29T11:57:31.221441Z",
     "iopub.status.idle": "2025-05-29T11:57:31.399348Z",
     "shell.execute_reply": "2025-05-29T11:57:31.398234Z",
     "shell.execute_reply.started": "2025-05-29T11:57:31.221698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 29 11:57:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0             28W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0f5ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:57:31.401066Z",
     "iopub.status.busy": "2025-05-29T11:57:31.400719Z",
     "iopub.status.idle": "2025-05-29T11:58:12.342804Z",
     "shell.execute_reply": "2025-05-29T11:58:12.341812Z",
     "shell.execute_reply.started": "2025-05-29T11:57:31.401028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a93928f9ae146bf801ab08ea0d2860d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b8a43691e146c2a1afd28c8afb01c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filtered_mp16.tar:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded dataset to /kaggle/working/G3-Original/data/mp16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5acf3523e045bab2383eeb72f9bb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MP16_Pro_filtered.csv:   0%|          | 0.00/747M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded metadata/MP16_Pro_filtered.csv to data/mp16/metadata/MP16_Pro_filtered.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4344cb52aef4ac8b5b6b85a7b658841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MP16_Pro_places365.csv:   0%|          | 0.00/859M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded metadata/MP16_Pro_places365.csv to data/mp16/metadata/MP16_Pro_places365.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd7d85fbbda4c7b9b112aa18f5d0a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mp16_urls.csv:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded metadata/mp16_urls.csv to data/mp16/metadata/mp16_urls.csv\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download, hf_hub_download, login\n",
    "\n",
    "login(token=\"\")\n",
    "\n",
    "path = snapshot_download(\n",
    "    repo_id=\"tduongvn/ACMMM25-Geolocation\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=[\"*.tar\"],\n",
    "    local_dir=\"data/mp16/\",\n",
    "    use_auth_token=True  # will use your local token from `huggingface-cli login`\n",
    ")\n",
    "print(f\"Downloaded dataset to {path}\")\n",
    "\n",
    "files = [\n",
    "    \"metadata/MP16_Pro_filtered.csv\",\n",
    "    \"metadata/MP16_Pro_places365.csv\",\n",
    "    \"metadata/mp16_urls.csv\"\n",
    "]\n",
    "for file in files:\n",
    "    path = hf_hub_download(\n",
    "        repo_id=\"Jia-py/MP16-Pro\",\n",
    "        filename=file,\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=\"data/mp16/\",\n",
    "        use_auth_token=True  # will use your local token from `huggingface-cli login`\n",
    "    )\n",
    "    print(f\"Downloaded {file} to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c729be-acf8-4be2-b6e9-2edf8023deba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:58:12.344027Z",
     "iopub.status.busy": "2025-05-29T11:58:12.343734Z",
     "iopub.status.idle": "2025-05-29T11:58:29.071613Z",
     "shell.execute_reply": "2025-05-29T11:58:29.070946Z",
     "shell.execute_reply.started": "2025-05-29T11:58:12.343995Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c409be5c2214a39b0a97c65ec11e5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eep_finetune_1epoch.pth:   0%|          | 0.00/1.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0689d418d663476b92fef5c83a9043ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "G3.index:   0%|          | 0.00/394M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'index/G3.index'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_hub_download(\n",
    "    repo_id=\"tduongvn/Checkpoints-ACMMM25\",\n",
    "    filename=\"eep_finetune_1epoch.pth\",\n",
    "    repo_type=\"model\",\n",
    "    local_dir=\"./checkpoints/\",\n",
    "    use_auth_token=True\n",
    ")\n",
    "hf_hub_download(\n",
    "    repo_id=\"tduongvn/Checkpoints-ACMMM25\",\n",
    "    filename=\"index/G3.index\",\n",
    "    repo_type=\"model\",\n",
    "    local_dir=\"./\",\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bffa1c2-c748-4e45-921e-f5d601e61fa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:58:29.072614Z",
     "iopub.status.busy": "2025-05-29T11:58:29.072367Z",
     "iopub.status.idle": "2025-05-29T11:58:29.197797Z",
     "shell.execute_reply": "2025-05-29T11:58:29.196289Z",
     "shell.execute_reply.started": "2025-05-29T11:58:29.072596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_G3_im2gps3k.npy\t   G3.index\t      I_G3_im2gps3k_reverse.npy\n",
      "D_G3_im2gps3k_reverse.npy  I_G3_im2gps3k.npy\n"
     ]
    }
   ],
   "source": [
    "!ls index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5bdea04-15b3-4fba-bbf3-2bee63c588fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:58:29.199129Z",
     "iopub.status.busy": "2025-05-29T11:58:29.198894Z",
     "iopub.status.idle": "2025-05-29T11:58:33.434747Z",
     "shell.execute_reply": "2025-05-29T11:58:33.433654Z",
     "shell.execute_reply.started": "2025-05-29T11:58:29.199105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mv data/mp16/metadata/*.csv data/mp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab080eb-9055-4d48-bf9f-f22e33e6d9bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:58:33.436378Z",
     "iopub.status.busy": "2025-05-29T11:58:33.436039Z",
     "iopub.status.idle": "2025-05-29T11:58:33.559515Z",
     "shell.execute_reply": "2025-05-29T11:58:33.558355Z",
     "shell.execute_reply.started": "2025-05-29T11:58:33.436348Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_mp16.tar  MP16_Pro_filtered.csv   mp16_urls.csv\n",
      "metadata\t   MP16_Pro_places365.csv\n"
     ]
    }
   ],
   "source": [
    "!ls data/mp16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf93114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:58:33.561151Z",
     "iopub.status.busy": "2025-05-29T11:58:33.560799Z",
     "iopub.status.idle": "2025-05-29T11:58:41.945190Z",
     "shell.execute_reply": "2025-05-29T11:58:41.944540Z",
     "shell.execute_reply.started": "2025-05-29T11:58:33.561112Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/lctngdng/im2gps3k\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = ''\n",
    "os.environ['KAGGLE_KEY'] = ''\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "api.dataset_download_files('lctngdng/im2gps3k', path='data/im2gps3k', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed67e52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:58:41.949352Z",
     "iopub.status.busy": "2025-05-29T11:58:41.949080Z",
     "iopub.status.idle": "2025-05-29T11:58:44.982023Z",
     "shell.execute_reply": "2025-05-29T11:58:44.981374Z",
     "shell.execute_reply.started": "2025-05-29T11:58:41.949335Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\n",
      "  warnings.warn(f'Downloading: {url}', DownloadWarning)\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import cartopy.io.shapereader as shpreader\n",
    "\n",
    "# Load Natural Earth country polygons\n",
    "shp_path = shpreader.natural_earth(\n",
    "    resolution='110m',\n",
    "    category='cultural',\n",
    "    name='admin_0_countries'\n",
    ")\n",
    "world = gpd.read_file(shp_path)[[\"ADMIN\", \"geometry\"]].to_crs(epsg=4326)\n",
    "# Build spatial index for performance\n",
    "sindex = world.sindex\n",
    "\n",
    "# Function returns country name for a given latitude/longitude\n",
    "def point_to_country(lat, lon, countries=world, index=sindex):\n",
    "    pt = Point(lon, lat)\n",
    "    # Find candidate polygons via spatial index\n",
    "    candidates = list(index.intersection(pt.bounds))\n",
    "    # Check strict land contains\n",
    "    for idx in candidates:\n",
    "        if countries.geometry.iloc[idx].contains(pt):\n",
    "            return countries.ADMIN.iloc[idx]\n",
    "    # Fallback to intersects for border/water cases\n",
    "    for idx in candidates:\n",
    "        if countries.geometry.iloc[idx].intersects(pt):\n",
    "            return countries.ADMIN.iloc[idx]\n",
    "    # No match found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "027ca254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:58:44.983212Z",
     "iopub.status.busy": "2025-05-29T11:58:44.982907Z",
     "iopub.status.idle": "2025-05-29T11:58:45.023910Z",
     "shell.execute_reply": "2025-05-29T11:58:45.023144Z",
     "shell.execute_reply.started": "2025-05-29T11:58:44.983185Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMG_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>S3_Label</th>\n",
       "      <th>S16_Label</th>\n",
       "      <th>S365_Label</th>\n",
       "      <th>Prob_indoor</th>\n",
       "      <th>Prob_natural</th>\n",
       "      <th>Prob_urban</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000269685_e60e9cdfb4_1125_78841376@N00.jpg</td>\n",
       "      <td>78841376@N00</td>\n",
       "      <td>32.325436</td>\n",
       "      <td>-64.764404</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>353</td>\n",
       "      <td>0.274242</td>\n",
       "      <td>0.045113</td>\n",
       "      <td>0.680645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000304467_1a75a200b1_1296_78841376@N00.jpg</td>\n",
       "      <td>78841376@N00</td>\n",
       "      <td>32.325436</td>\n",
       "      <td>-64.764404</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>325</td>\n",
       "      <td>0.414407</td>\n",
       "      <td>0.220912</td>\n",
       "      <td>0.364681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001048550_8e4b47d165_1051_78841376@N00.jpg</td>\n",
       "      <td>78841376@N00</td>\n",
       "      <td>32.325436</td>\n",
       "      <td>-64.764404</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.969903</td>\n",
       "      <td>0.022771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1005977048_5ccf8b05d3_1201_91728102@N00.jpg</td>\n",
       "      <td>91728102@N00</td>\n",
       "      <td>29.976052</td>\n",
       "      <td>122.390356</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>273</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.476665</td>\n",
       "      <td>0.519468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1008804117_ce4e6fef8a_1349_97522422@N00.jpg</td>\n",
       "      <td>97522422@N00</td>\n",
       "      <td>46.478536</td>\n",
       "      <td>30.758714</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>0.973878</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>0.023670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        IMG_ID        AUTHOR        LAT  \\\n",
       "0  1000269685_e60e9cdfb4_1125_78841376@N00.jpg  78841376@N00  32.325436   \n",
       "1  1000304467_1a75a200b1_1296_78841376@N00.jpg  78841376@N00  32.325436   \n",
       "2  1001048550_8e4b47d165_1051_78841376@N00.jpg  78841376@N00  32.325436   \n",
       "3  1005977048_5ccf8b05d3_1201_91728102@N00.jpg  91728102@N00  29.976052   \n",
       "4  1008804117_ce4e6fef8a_1349_97522422@N00.jpg  97522422@N00  46.478536   \n",
       "\n",
       "          LON  S3_Label  S16_Label  S365_Label  Prob_indoor  Prob_natural  \\\n",
       "0  -64.764404         2         12         353     0.274242      0.045113   \n",
       "1  -64.764404         0          4         325     0.414407      0.220912   \n",
       "2  -64.764404         1          8          36     0.007326      0.969903   \n",
       "3  122.390356         2          6         273     0.003868      0.476665   \n",
       "4   30.758714         0          0         198     0.973878      0.002453   \n",
       "\n",
       "   Prob_urban  \n",
       "0    0.680645  \n",
       "1    0.364681  \n",
       "2    0.022771  \n",
       "3    0.519468  \n",
       "4    0.023670  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/im2gps3k/im2gps3k_places365.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75c7e87a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:58:45.025321Z",
     "iopub.status.busy": "2025-05-29T11:58:45.025013Z",
     "iopub.status.idle": "2025-05-29T11:58:46.796704Z",
     "shell.execute_reply": "2025-05-29T11:58:46.795775Z",
     "shell.execute_reply.started": "2025-05-29T11:58:45.025291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def is_target_country(row):\n",
    "    lat, lon = row[\"LAT\"], row[\"LON\"]\n",
    "    return point_to_country(lat, lon) in [\"Ukraine\", \"Israel\", \"Russia\", \"Palestine\"]\n",
    "\n",
    "# apply row‐wise and get a boolean mask\n",
    "mask = df.apply(is_target_country, axis=1)\n",
    "\n",
    "# filter the dataframe\n",
    "df_filtered = df[mask]\n",
    "df_filtered.head()\n",
    "df_filtered.to_csv(\"data/im2gps3k/im2gps3k_places365.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97e0609d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:58:46.798037Z",
     "iopub.status.busy": "2025-05-29T11:58:46.797742Z",
     "iopub.status.idle": "2025-05-29T11:58:46.818296Z",
     "shell.execute_reply": "2025-05-29T11:58:46.817286Z",
     "shell.execute_reply.started": "2025-05-29T11:58:46.798007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "Path(\"data/im2gps3k/filtered_im2gps3k\").mkdir(parents=True, exist_ok=True)\n",
    "for image in df_filtered[\"IMG_ID\"]:\n",
    "    shutil.copyfile(\n",
    "        f\"data/im2gps3k/im2gps3ktest/{image}\",\n",
    "        f\"data/im2gps3k/filtered_im2gps3k/{image}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb61ccfd-3a77-46bf-9f18-e7d852f07f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T04:40:45.289371Z",
     "iopub.status.busy": "2025-05-29T04:40:45.288449Z",
     "iopub.status.idle": "2025-05-29T04:40:45.325498Z",
     "shell.execute_reply": "2025-05-29T04:40:45.324881Z",
     "shell.execute_reply.started": "2025-05-29T04:40:45.289345Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import base64\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from google import genai\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def get_response(image_path, base_url, api_key, model_name, detail=\"low\", max_tokens=200, temperature=1.2, n=10):\n",
    "    \"\"\"Sends an image to the Gemini API and returns the geolocation response.\"\"\"\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    my_file = client.files.upload(file=image_path)\n",
    "\n",
    "    prompt = \"\"\"Suppose you are an expert in geo-localization, you have the ability to give two number GPS coordination given an image.\n",
    "                 Please give me the location of the given image.\n",
    "                 Remember, you must have an answer, just output your best guess, don't answer me that you can't give a location.\n",
    "                 Your answer should be in the following JSON format without any other information: {\"latitude\": float,\"longitude\": float}.\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[my_file, prompt],\n",
    "    )\n",
    "    \n",
    "    print(response.text)\n",
    "    try:\n",
    "        return response.text\n",
    "    except KeyError:\n",
    "        return ['{\"latitude\": 0.0, \"longitude\": 0.0}']\n",
    "\n",
    "def get_response_rag(image_path, base_url, api_key, model_name, candidates_gps, reverse_gps, detail=\"low\", max_tokens=200, temperature=1.2, n=10):\n",
    "    \"\"\"Sends an image to the Gemini API and returns the geolocation response.\"\"\"\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    my_file = client.files.upload(file=image_path)\n",
    "\n",
    "    prompt = f\"\"\"Suppose you are an expert in geo-localization, Please analyze this image and give me a guess of the location.\n",
    "                Your answer must be to the coordinates level in (latitude, longitude) format.\n",
    "                For your reference, these are coordinates of some similar images: {candidates_gps}, and these are coordinates of some dissimilar images: {reverse_gps}.\n",
    "                Remember, you must have an answer, just output your best guess, don't answer me that you can't give an location.\n",
    "                Your answer should be in the following JSON format without any other information: {{\"latitude\": float,\"longitude\": float}}.\n",
    "                Your answer should be in the following JSON format without any other information: {{\"latitude\": float,\"longitude\": float}}.\n",
    "                \"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[my_file, prompt],\n",
    "    )\n",
    "    \n",
    "    print(response.text)\n",
    "    try:\n",
    "        return response.text\n",
    "    except KeyError:\n",
    "        return ['{\"latitude\": 0.0, \"longitude\": 0.0}']\n",
    "\n",
    "\n",
    "def process_row(row, base_url, api_key, model_name, root_path, image_path):\n",
    "    image_path = os.path.join(root_path, image_path, row[\"IMG_ID\"])\n",
    "    try:\n",
    "        response = get_response(image_path, base_url, api_key, model_name)\n",
    "    except Exception as e:\n",
    "        response = \"None\"\n",
    "        print(e)\n",
    "    row['response'] = response\n",
    "    return row\n",
    "\n",
    "def process_row_rag(row, base_url, api_key, model_name, root_path, image_path, rag_sample_num):\n",
    "    image_path = os.path.join(root_path, image_path, row[\"IMG_ID\"])\n",
    "    try:\n",
    "        #candidates_gps = [eval(row[f'candidate_{i}_gps']) for i in range(rag_sample_num)]\n",
    "        candidates_gps = [row[f'candidate_{i}_gps'] for i in range(rag_sample_num)]\n",
    "        candidates_gps = str(candidates_gps)\n",
    "        #reverse_gps = [eval(row[f'reverse_{i}_gps']) for i in range(rag_sample_num)]\n",
    "        reverse_gps = [row[f'reverse_{i}_gps'] for i in range(rag_sample_num)]\n",
    "        reverse_gps = str(reverse_gps)\n",
    "        response = get_response_rag(image_path, base_url, api_key, model_name, candidates_gps, reverse_gps)\n",
    "    except Exception as e:\n",
    "        response = \"None\"\n",
    "        print(e)\n",
    "    row['rag_response'] = response\n",
    "    return row\n",
    "\n",
    "def check_conditions(coord_str):\n",
    "    if coord_str.startswith('[]') or coord_str.startswith('None'):\n",
    "        return True\n",
    "    try:\n",
    "        coordinates = ast.literal_eval(coord_str)\n",
    "        return float(coordinates[0]) == 0.0\n",
    "    except:\n",
    "        return False\n",
    "        \n",
    "def clean_and_load_json(response_str):\n",
    "    s = response_str.strip()\n",
    "    if s.startswith(\"```json\") or s.startswith(\"```\"):\n",
    "        # Remove leading ```json or ``` and trailing ```\n",
    "        s = re.sub(r'^```json\\s*|^```\\s*|\\s*```$', '', s, flags=re.DOTALL)\n",
    "    return json.loads(s)\n",
    "    \n",
    "def run(args):\n",
    "    api_key = args.api_key\n",
    "    model_name = args.model_name\n",
    "    base_url = args.base_url\n",
    "    root_path = args.root_path\n",
    "    text_path = args.text_path\n",
    "    image_path = args.image_path\n",
    "    result_path = args.result_path\n",
    "    rag_path = args.rag_path\n",
    "    process = args.process\n",
    "    rag_sample_num = args.rag_sample_num\n",
    "    searching_file_name = args.searching_file_name\n",
    "\n",
    "    if process == 'predict':\n",
    "        if os.path.exists(os.path.join(root_path, result_path)):\n",
    "            df = pd.read_csv(os.path.join(root_path, result_path))\n",
    "            df_rerun = df[df['response'].isna()]\n",
    "            print('Need Rerun:', df_rerun.shape[0])\n",
    "            df_rerun = df_rerun.parallel_apply(lambda row: process_row(row, base_url, api_key, model_name, root_path, image_path), axis=1)\n",
    "            df.update(df_rerun)\n",
    "            df.to_csv(os.path.join(root_path, result_path), index=False)\n",
    "        else:\n",
    "            df = pd.read_csv(os.path.join(root_path, text_path))\n",
    "            df = df.parallel_apply(lambda row: process_row(row, base_url, api_key, model_name, root_path, image_path), axis=1)\n",
    "            df.to_csv(os.path.join(root_path, result_path), index=False)\n",
    "\n",
    "    if process == 'extract':\n",
    "        df = pd.read_csv(os.path.join(root_path, result_path))\n",
    "        pattern = r'[-+]?\\d+\\.\\d+'\n",
    "        df['coordinates'] = df['response'].apply(lambda x: re.findall(pattern, x))\n",
    "        df.to_csv(os.path.join(root_path, result_path), index=False)\n",
    "        \n",
    "    if process == 'rag':\n",
    "        database_df = pd.read_csv('./data/mp16/MP16_Pro_filtered.csv')\n",
    "        if not os.path.exists(os.path.join(root_path, str(rag_sample_num) + '_' + rag_path)):\n",
    "            df = pd.read_csv(os.path.join(root_path, text_path))\n",
    "            I = np.load('./index/{}.npy'.format(searching_file_name))\n",
    "            reverse_I = np.load('./index/{}_reverse.npy'.format(searching_file_name))\n",
    "            for i in tqdm(range(df.shape[0])):\n",
    "                candidate_idx_lis = I[i]\n",
    "                candidate_gps = database_df.loc[candidate_idx_lis, ['LAT', 'LON', 'city', 'state', 'country']].values\n",
    "                for idx, (latitude, longitude, city, state, country) in enumerate(candidate_gps):\n",
    "                    df.loc[i, f'candidate_{idx}_gps'] = f'[{latitude}, {longitude}]'\n",
    "                reverse_idx_lis = reverse_I[i]\n",
    "                reverse_gps = database_df.loc[reverse_idx_lis, ['LAT', 'LON', 'city', 'state', 'country']].values\n",
    "                for idx, (latitude, longitude, city, state, country) in enumerate(reverse_gps):\n",
    "                    df.loc[i, f'reverse_{idx}_gps'] = f'[{latitude}, {longitude}]'\n",
    "            df.to_csv(os.path.join(root_path, str(rag_sample_num) + '_' + rag_path), index=False)\n",
    "            df = df.parallel_apply(lambda row: process_row_rag(row, base_url, api_key, model_name, root_path, image_path, rag_sample_num), axis=1)\n",
    "            df.to_csv(os.path.join(root_path, str(rag_sample_num) + '_' + rag_path), index=False)\n",
    "        else:\n",
    "            df = pd.read_csv(os.path.join(root_path, str(rag_sample_num) + '_' + rag_path))\n",
    "            # df_rerun = df[df['rag_coordinates'].apply(check_conditions)]\n",
    "            df_rerun = df[(df['rag_response'].isna()) | (df['rag_response'] == '``json')]\n",
    "            print('Need Rerun:', df_rerun.shape[0])\n",
    "            df_rerun = df_rerun.parallel_apply(lambda row: process_row_rag(row, base_url, api_key, model_name, root_path, image_path, rag_sample_num), axis=1)\n",
    "            df.update(df_rerun)\n",
    "            df.to_csv(os.path.join(root_path,  str(rag_sample_num) + '_' + rag_path), index=False)\n",
    "            \n",
    "    if process == 'rag_extract':\n",
    "        df = pd.read_csv(os.path.join(root_path,  str(rag_sample_num) + '_' + rag_path)).fillna(\"None\")\n",
    "        pattern = r'[-+]?\\d+\\.\\d+'\n",
    "        df['rag_coordinates'] = df['rag_response'].apply(lambda x: re.findall(pattern, x))\n",
    "        df.to_csv(os.path.join(root_path,  str(rag_sample_num) + '_' + rag_path), index=False)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Args:\n",
    "        api_key = \"\"\n",
    "        \n",
    "        model_name = \"gemini-2.0-flash\"\n",
    "        base_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyAIsZFj2ahstYe-cgxUaAUWyKq7Fg4Tzy8\" \n",
    "        root_path = \"./data/im2gps3k\"\n",
    "        text_path = \"im2gps3k_places365.csv\"\n",
    "        image_path = \"im2gps3ktest\"\n",
    "        result_path = \"llm_predict_results_zs.csv\"\n",
    "        rag_path = \"llm_predict_results_rag.csv\"\n",
    "        process = 'rag_extract'  # Options: 'predict', 'extract', 'rag', 'rag_extract'\n",
    "        rag_sample_num = 5\n",
    "        searching_file_name = 'I_G3_im2gps3k'\n",
    "\n",
    "    pandarallel.initialize(progress_bar=True, nb_workers=4)\n",
    "    run(Args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26a61d96-dec0-44c5-8346-c2062abce89c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T05:33:55.571512Z",
     "iopub.status.busy": "2025-05-29T05:33:55.571115Z",
     "iopub.status.idle": "2025-05-29T05:33:55.702008Z",
     "shell.execute_reply": "2025-05-29T05:33:55.701394Z",
     "shell.execute_reply.started": "2025-05-29T05:33:55.571458Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:00<00:00, 3084.59it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 3345.55it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 3542.99it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 3486.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "df_raw = pd.read_csv('./data/im2gps3k/im2gps3k_places365.csv')\n",
    "zs_df = pd.read_csv('./data/im2gps3k/llm_predict_results_zs.csv')\n",
    "rag_5_df = pd.read_csv('./data/im2gps3k/5_llm_predict_results_rag.csv')\n",
    "rag_10_df = pd.read_csv('./data/im2gps3k/10_llm_predict_results_rag.csv')\n",
    "rag_15_df = pd.read_csv('./data/im2gps3k/15_llm_predict_results_rag.csv')\n",
    "\n",
    "pattern = r'[-+]?\\d+\\.\\d+'\n",
    "idx = 0\n",
    "for i in tqdm(range(df_raw.shape[0])):\n",
    "    response = zs_df.loc[i, 'coordinates']\n",
    "\n",
    "    # If response is a string, try to evaluate it\n",
    "    if isinstance(response, str):\n",
    "        try:\n",
    "            match = ast.literal_eval(response)\n",
    "        except:\n",
    "            match = []\n",
    "    else:\n",
    "        match = response  # already a list or dict\n",
    "\n",
    "    try:\n",
    "        # If it's a dict like {'latitude': ..., 'longitude': ...}\n",
    "        if isinstance(match, dict):\n",
    "            latitude = match.get('latitude', 0.0)\n",
    "            longitude = match.get('longitude', 0.0)\n",
    "        else:\n",
    "            # Assume list format like ['43.1742', '131.8617']\n",
    "            latitude = match[0]\n",
    "            longitude = match[1]\n",
    "    except:\n",
    "        latitude = '0.0'\n",
    "        longitude = '0.0'\n",
    "\n",
    "    df_raw.loc[i, f'zs_rag_{idx}_latitude'] = latitude\n",
    "    df_raw.loc[i, f'zs_rag_{idx}_longitude'] = longitude\n",
    "\n",
    "\n",
    "for i in tqdm(range(df_raw.shape[0])):\n",
    "    response = rag_5_df.loc[i, 'rag_coordinates']\n",
    "\n",
    "    # If response is a string, try to evaluate it\n",
    "    if isinstance(response, str):\n",
    "        try:\n",
    "            match = ast.literal_eval(response)\n",
    "        except:\n",
    "            match = []\n",
    "    else:\n",
    "        match = response  # already a list or dict\n",
    "\n",
    "    try:\n",
    "        # If it's a dict like {'latitude': ..., 'longitude': ...}\n",
    "        if isinstance(match, dict):\n",
    "            latitude = match.get('latitude', 0.0)\n",
    "            longitude = match.get('longitude', 0.0)\n",
    "        else:\n",
    "            # Assume list format like ['43.1742', '131.8617']\n",
    "            latitude = match[0]\n",
    "            longitude = match[1]\n",
    "    except:\n",
    "        latitude = '0.0'\n",
    "        longitude = '0.0'\n",
    "\n",
    "    df_raw.loc[i, f'5_rag_{idx}_latitude'] = latitude\n",
    "    df_raw.loc[i, f'5_rag_{idx}_longitude'] = longitude\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(df_raw.shape[0])):\n",
    "    response = rag_10_df.loc[i, 'rag_coordinates']\n",
    "\n",
    "    # If response is a string, try to evaluate it\n",
    "    if isinstance(response, str):\n",
    "        try:\n",
    "            match = ast.literal_eval(response)\n",
    "        except:\n",
    "            match = []\n",
    "    else:\n",
    "        match = response  # already a list or dict\n",
    "\n",
    "    try:\n",
    "        # If it's a dict like {'latitude': ..., 'longitude': ...}\n",
    "        if isinstance(match, dict):\n",
    "            latitude = match.get('latitude', 0.0)\n",
    "            longitude = match.get('longitude', 0.0)\n",
    "        else:\n",
    "            # Assume list format like ['43.1742', '131.8617']\n",
    "            latitude = match[0]\n",
    "            longitude = match[1]\n",
    "    except:\n",
    "        latitude = '0.0'\n",
    "        longitude = '0.0'\n",
    "\n",
    "    df_raw.loc[i, f'10_rag_{idx}_latitude'] = latitude\n",
    "    df_raw.loc[i, f'10_rag_{idx}_longitude'] = longitude\n",
    "\n",
    "\n",
    "for i in tqdm(range(df_raw.shape[0])):\n",
    "    response = rag_15_df.loc[i, 'rag_coordinates']\n",
    "\n",
    "    # If response is a string, try to evaluate it\n",
    "    if isinstance(response, str):\n",
    "        try:\n",
    "            match = ast.literal_eval(response)\n",
    "        except:\n",
    "            match = []\n",
    "    else:\n",
    "        match = response  # already a list or dict\n",
    "\n",
    "    try:\n",
    "        # If it's a dict like {'latitude': ..., 'longitude': ...}\n",
    "        if isinstance(match, dict):\n",
    "            latitude = match.get('latitude', 0.0)\n",
    "            longitude = match.get('longitude', 0.0)\n",
    "        else:\n",
    "            # Assume list format like ['43.1742', '131.8617']\n",
    "            latitude = match[0]\n",
    "            longitude = match[1]\n",
    "    except:\n",
    "        latitude = '0.0'\n",
    "        longitude = '0.0'\n",
    "\n",
    "    df_raw.loc[i, f'15_rag_{idx}_latitude'] = latitude\n",
    "    df_raw.loc[i, f'15_rag_{idx}_longitude'] = longitude\n",
    "df_raw.to_csv('./data/im2gps3k/im2gps3k_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c34d9241-1f2e-4cba-8e5f-f9a3c2bd2a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T11:58:46.819598Z",
     "iopub.status.busy": "2025-05-29T11:58:46.819279Z",
     "iopub.status.idle": "2025-05-29T11:59:11.321160Z",
     "shell.execute_reply": "2025-05-29T11:59:11.320511Z",
     "shell.execute_reply.started": "2025-05-29T11:58:46.819572Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 11:58:56.076977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748519936.285456      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748519936.345017      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import ast\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from geopy.distance import geodesic\n",
    "from transformers import CLIPImageProcessor, CLIPModel\n",
    "from utils.utils import MP16Dataset, im2gps3kDataset, yfcc4kDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e07468dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T12:01:59.392899Z",
     "iopub.status.busy": "2025-05-29T12:01:59.392082Z",
     "iopub.status.idle": "2025-05-29T12:01:59.416436Z",
     "shell.execute_reply": "2025-05-29T12:01:59.415684Z",
     "shell.execute_reply.started": "2025-05-29T12:01:59.392858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class GeoImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_folder, topn, vision_processor, database_df, I):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_folder = img_folder\n",
    "        self.topn = topn\n",
    "        self.vision_processor = vision_processor\n",
    "        self.database_df = database_df\n",
    "        self.I = I\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f'{self.img_folder}/{self.dataframe.loc[idx, \"IMG_ID\"]}'\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.vision_processor(images=image, return_tensors='pt')['pixel_values'].reshape(3,224,224)\n",
    "        \n",
    "        gps_data = []\n",
    "        search_top1_latitude, search_top1_longitude = self.database_df.loc[self.I[idx][0], ['LAT', 'LON']].values\n",
    "        rag_5, rag_10, rag_15, zs = [],[],[],[]\n",
    "        for j in range(self.topn):\n",
    "            gps_data.extend([\n",
    "                float(self.dataframe.loc[idx, f'5_rag_{j}_latitude']),\n",
    "                float(self.dataframe.loc[idx, f'5_rag_{j}_longitude']),\n",
    "                float(self.dataframe.loc[idx, f'10_rag_{j}_latitude']),\n",
    "                float(self.dataframe.loc[idx, f'10_rag_{j}_longitude']),\n",
    "                float(self.dataframe.loc[idx, f'15_rag_{j}_latitude']),\n",
    "                float(self.dataframe.loc[idx, f'15_rag_{j}_longitude']),\n",
    "                float(self.dataframe.loc[idx, f'zs_rag_{j}_latitude']),\n",
    "                float(self.dataframe.loc[idx, f'zs_rag_{j}_longitude']),\n",
    "                search_top1_latitude,\n",
    "                search_top1_longitude\n",
    "            ])\n",
    "        \n",
    "        gps_data = np.array(gps_data).reshape(-1, 2)\n",
    "        return image, gps_data, idx\n",
    "\n",
    "def evaluate(args, I):\n",
    "    print('start evaluation')\n",
    "    if args.database == 'mp16':\n",
    "        database = args.database_df\n",
    "        df = args.dataset_df\n",
    "        df['NN_idx'] = I[:, 0]\n",
    "        df['LAT_pred'] = df.apply(lambda x: database.loc[x['NN_idx'],'LAT'], axis=1)\n",
    "        df['LON_pred'] = df.apply(lambda x: database.loc[x['NN_idx'],'LON'], axis=1)\n",
    "\n",
    "        df_llm = pd.read_csv(f'./data/{args.dataset}/{args.dataset}_prediction.csv')\n",
    "        print(df_llm.columns.tolist())  # Make sure it contains '5_rag_0_latitude'\n",
    "        model = torch.load('./checkpoints/eep_finetune_1epoch.pth', map_location='cuda:0')\n",
    "        topn = 1 # number of candidates\n",
    "\n",
    "        dataset = GeoImageDataset(df_llm, f'./data/{args.dataset}/im2gps3ktest', topn, vision_processor=model.vision_processor, database_df=database, I=I)\n",
    "        data_loader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        for images, gps_batch, indices in tqdm(data_loader):\n",
    "            images = images.to(args.device)\n",
    "            image_embeds = model.vision_projection_else_2(model.vision_projection(model.vision_model(images)[1]))\n",
    "            image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True) # b, 768\n",
    "\n",
    "            gps_batch = gps_batch.to(args.device)\n",
    "            gps_input = gps_batch.clone().detach()\n",
    "            b, c, _ = gps_input.shape\n",
    "            gps_input = gps_input.reshape(b*c, 2)\n",
    "            location_embeds = model.location_encoder(gps_input)\n",
    "            location_embeds = model.location_projection_else(location_embeds.reshape(b*c, -1))\n",
    "            location_embeds = location_embeds / location_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "            location_embeds = location_embeds.reshape(b, c, -1) #  b, c, 768\n",
    "\n",
    "            similarity = torch.matmul(image_embeds.unsqueeze(1), location_embeds.permute(0, 2, 1)) # b, 1, c\n",
    "            similarity = similarity.squeeze(1).cpu().detach().numpy()\n",
    "            max_idxs = np.argmax(similarity, axis=1)\n",
    "            \n",
    "            # update DataFrame\n",
    "            for i, max_idx in enumerate(max_idxs):\n",
    "                final_idx = indices[i]\n",
    "                final_idx = final_idx.item()\n",
    "                final_latitude, final_longitude = gps_batch[i][max_idx]\n",
    "                final_latitude, final_longitude = final_latitude.item(), final_longitude.item()\n",
    "                if final_latitude < -90 or final_latitude > 90:\n",
    "                    final_latitude = 0\n",
    "                if final_longitude < -180 or final_longitude > 180:\n",
    "                    final_longitude = 0\n",
    "                df.loc[final_idx, 'LAT_pred'] = final_latitude\n",
    "                df.loc[final_idx, 'LON_pred'] = final_longitude\n",
    "\n",
    "        df['geodesic'] = df.apply(lambda x: geodesic((x['LAT'], x['LON']), (x['LAT_pred'], x['LON_pred'])).km, axis=1)\n",
    "        print(df.head())\n",
    "        df.to_csv(f'./data/{args.dataset}_{args.index}_results.csv', index=False)\n",
    "\n",
    "        # 1, 25, 200, 750, 2500 km level\n",
    "        print('2500km level: ', df[df['geodesic'] < 2500].shape[0] / df.shape[0])\n",
    "        print('750km level: ', df[df['geodesic'] < 750].shape[0] / df.shape[0])\n",
    "        print('200km level: ', df[df['geodesic'] < 200].shape[0] / df.shape[0])\n",
    "        print('25km level: ', df[df['geodesic'] < 25].shape[0] / df.shape[0])\n",
    "        print('1km level: ', df[df['geodesic'] < 1].shape[0] / df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88020d45-fb76-494f-b801-07c313d00472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T12:02:04.325638Z",
     "iopub.status.busy": "2025-05-29T12:02:04.325297Z",
     "iopub.status.idle": "2025-05-29T12:02:32.160817Z",
     "shell.execute_reply": "2025-05-29T12:02:32.159865Z",
     "shell.execute_reply.started": "2025-05-29T12:02:04.325609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluation\n",
      "['Unnamed: 0', 'IMG_ID', 'AUTHOR', 'LAT', 'LON', 'S3_Label', 'S16_Label', 'S365_Label', 'Prob_indoor', 'Prob_natural', 'Prob_urban', 'zs_rag_0_latitude', 'zs_rag_0_longitude', '5_rag_0_latitude', '5_rag_0_longitude', '10_rag_0_latitude', '10_rag_0_longitude', '15_rag_0_latitude', '15_rag_0_longitude']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/1505978896.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('./checkpoints/eep_finetune_1epoch.pth', map_location='cuda:0')\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                       IMG_ID        AUTHOR  \\\n",
      "0          58  1076964851_c20355b981_1431_93455345@N00.jpg  93455345@N00   \n",
      "1          59  1076986527_4de9d086bc_1124_93455345@N00.jpg  93455345@N00   \n",
      "2          60  1077074655_69c5dba648_1048_93455345@N00.jpg  93455345@N00   \n",
      "3          61  1077086393_e1aea6d21e_1090_93455345@N00.jpg  93455345@N00   \n",
      "4          62  1077155473_80c037f16a_1120_93455345@N00.jpg  93455345@N00   \n",
      "\n",
      "         LAT         LON  S3_Label  S16_Label  S365_Label  Prob_indoor  \\\n",
      "0  43.137069  131.929321         2         15         178     0.034063   \n",
      "1  43.137069  131.929321         2         14         201     0.070768   \n",
      "2  52.045734  113.411865         2         11         226     0.046202   \n",
      "3  52.321910  104.227294         2         10         174     0.018314   \n",
      "4  51.733832   39.254150         2         10         175     0.002522   \n",
      "\n",
      "   Prob_natural  Prob_urban  NN_idx   LAT_pred    LON_pred     geodesic  \n",
      "0      0.101330    0.864607    2624  43.174200  131.861700     6.875014  \n",
      "1      0.314750    0.614482   22442  44.800003   20.466667  8139.278675  \n",
      "2      0.049423    0.904374   33428  56.837500   60.613900  3379.087538  \n",
      "3      0.069934    0.911752   10370  51.513500  104.156400    90.080521  \n",
      "4      0.211877    0.785602   10789  52.232200   21.008400  1251.464164  \n",
      "2500km level:  0.847457627118644\n",
      "750km level:  0.6949152542372882\n",
      "200km level:  0.6271186440677966\n",
      "25km level:  0.5932203389830508\n",
      "1km level:  0.3220338983050847\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.index = 'G3'\n",
    "        self.dataset = 'im2gps3k'  # or 'yfcc4k'\n",
    "        self.database = 'mp16'\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        if self.dataset == 'im2gps3k':\n",
    "            self.dataset_df = pd.read_csv('./data/im2gps3k/im2gps3k_places365.csv')\n",
    "        elif self.dataset == 'yfcc4k':\n",
    "            self.dataset_df = pd.read_csv('./data/yfcc4k/yfcc4k_places365.csv')\n",
    "\n",
    "        if self.database == 'mp16':\n",
    "            self.database_df = pd.read_csv('./data/mp16/MP16_Pro_filtered.csv')\n",
    "\n",
    "\n",
    "# Simulate command-line args\n",
    "args = Args()\n",
    "\n",
    "# Initialize FAISS GPU resources\n",
    "res = faiss.StandardGpuResources()\n",
    "\n",
    "# Ensure index folder exists\n",
    "if not os.path.exists('./index'):\n",
    "    os.makedirs('./index')\n",
    "\n",
    "index_path = f'./index/{args.index}.index'\n",
    "D_path = f'./index/D_{args.index}_{args.dataset}.npy'\n",
    "I_path = f'./index/I_{args.index}_{args.dataset}.npy'\n",
    "\n",
    "# Build or load index and evaluate\n",
    "if not os.path.exists(index_path):\n",
    "    build_index(args)\n",
    "else:\n",
    "    if not os.path.exists(I_path):\n",
    "        index = faiss.read_index(index_path)\n",
    "        print('read index success')\n",
    "        D, I = search_index(args, index, 20)\n",
    "        np.save(D_path, D)\n",
    "        np.save(I_path, I)\n",
    "    else:\n",
    "        D = np.load(D_path)\n",
    "        I = np.load(I_path)\n",
    "\n",
    "    evaluate(args, I)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3238926,
     "sourceId": 5632975,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7541158,
     "sourceId": 11989575,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
