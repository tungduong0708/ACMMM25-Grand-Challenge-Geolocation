{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install transformers accelerate huggingface_hub pandas optuna pyproj einops geopy matplotlib kaggle  geopandas cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d015e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, hf_hub_download, login\n",
    "\n",
    "login(token=\"API_KEY\")\n",
    "\n",
    "path = snapshot_download(\n",
    "    repo_id=\"tduongvn/ACMMM25-Geolocation\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=[\"*.tar\"],\n",
    "    local_dir=\"data/mp16/\",\n",
    "    use_auth_token=True  # will use your local token from `huggingface-cli login`\n",
    ")\n",
    "print(f\"Downloaded dataset to {path}\")\n",
    "\n",
    "files = [\n",
    "    \"metadata/MP16_Pro_filtered.csv\",\n",
    "    \"metadata/MP16_Pro_places365.csv\",\n",
    "    \"metadata/mp16_urls.csv\"\n",
    "]\n",
    "for file in files:\n",
    "    path = hf_hub_download(\n",
    "        repo_id=\"Jia-py/MP16-Pro\",\n",
    "        filename=file,\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=\"data/mp16/\",\n",
    "        use_auth_token=True  # will use your local token from `huggingface-cli login`\n",
    "    )\n",
    "    print(f\"Downloaded {file} to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff945a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv data/mp16/metadata/*.csv data/mp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = ''\n",
    "os.environ['KAGGLE_KEY'] = ''\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "api.dataset_download_files('lctngdng/im2gps3k', path='data/im2gps3k', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56837a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import cartopy.io.shapereader as shpreader\n",
    "\n",
    "# Load Natural Earth country polygons\n",
    "shp_path = shpreader.natural_earth(\n",
    "    resolution='110m',\n",
    "    category='cultural',\n",
    "    name='admin_0_countries'\n",
    ")\n",
    "world = gpd.read_file(shp_path)[[\"ADMIN\", \"geometry\"]].to_crs(epsg=4326)\n",
    "# Build spatial index for performance\n",
    "sindex = world.sindex\n",
    "\n",
    "# Function returns country name for a given latitude/longitude\n",
    "def point_to_country(lat, lon, countries=world, index=sindex):\n",
    "    pt = Point(lon, lat)\n",
    "    # Find candidate polygons via spatial index\n",
    "    candidates = list(index.intersection(pt.bounds))\n",
    "    # Check strict land contains\n",
    "    for idx in candidates:\n",
    "        if countries.geometry.iloc[idx].contains(pt):\n",
    "            return countries.ADMIN.iloc[idx]\n",
    "    # Fallback to intersects for border/water cases\n",
    "    for idx in candidates:\n",
    "        if countries.geometry.iloc[idx].intersects(pt):\n",
    "            return countries.ADMIN.iloc[idx]\n",
    "    # No match found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1830ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/im2gps3k/im2gps3k_places365.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_target_country(row):\n",
    "    lat, lon = row[\"LAT\"], row[\"LON\"]\n",
    "    return point_to_country(lat, lon) in [\"Ukraine\", \"Israel\", \"Russia\", \"Palestine\"]\n",
    "\n",
    "# apply row‚Äêwise and get a boolean mask\n",
    "mask = df.apply(is_target_country, axis=1)\n",
    "\n",
    "# filter the dataframe\n",
    "df_filtered = df[mask]\n",
    "df_filtered.head()\n",
    "df_filtered.to_csv(\"data/im2gps3k/im2gps3k_places365.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "Path(\"data/im2gps3k/filtered_im2gps3k\").mkdir(parents=True, exist_ok=True)\n",
    "for image in df_filtered[\"IMG_ID\"]:\n",
    "    shutil.copyfile(\n",
    "        f\"data/im2gps3k/im2gps3ktest/{image}\",\n",
    "        f\"data/im2gps3k/filtered_im2gps3k/{image}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252414ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "# Third-party libraries\n",
    "import torch\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "\n",
    "# Local application imports\n",
    "from utils.utils import MP16Dataset\n",
    "from utils.G3 import G3\n",
    "from zeroshot_prediction import ZeroShotPredictor\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def train_1epoch(dataloader, eval_dataloader, earlystopper, model, vision_processor, text_processor, optimizer, scheduler, device, accelerator=None):\n",
    "    model.train()\n",
    "    t = tqdm(dataloader, disable=not accelerator.is_local_main_process)\n",
    "    for i, (images, texts, longitude, latitude) in enumerate(t):\n",
    "        texts = text_processor(text=texts, padding='max_length', truncation=True, return_tensors='pt', max_length=77)\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        longitude = longitude.to(device).float()\n",
    "        latitude = latitude.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(images, texts, longitude, latitude, return_loss=True)\n",
    "        loss = output['loss']\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        if i % 1 == 0:\n",
    "            t.set_description('step {}, loss {}, lr {}'.format(i, loss.item(), scheduler.get_last_lr()[0]))\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ff9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])\n",
    "\n",
    "# fine-tune\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = 'cpu'\n",
    "hparams = yaml.safe_load(open('hparams.yaml', 'r'))\n",
    "pe = \"projection_mercator\"\n",
    "nn = \"rffmlp\"\n",
    "model = G3(\n",
    "    device=device,\n",
    "    positional_encoding_type=pe,\n",
    "    neural_network_type=nn,\n",
    "    hparams=hparams[f'{pe}_{nn}'],\n",
    ").to(device)\n",
    "\n",
    "# model = torch.load('g3_5_.pth')\n",
    "# location_encoder_dict = torch.load('checkpoints/location_encoder_weights.pth') # from geoclip\n",
    "# model.location_encoder.load_state_dict(location_encoder_dict)\n",
    "\n",
    "dataset = MP16Dataset(vision_processor = model.vision_processor, text_processor = model.text_processor, root_path='data/mp16/',image_data_path='filtered_mp16.tar')\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=16, pin_memory=True, prefetch_factor=5)\n",
    "\n",
    "\n",
    "params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.size())\n",
    "        params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([param for name,param in model.named_parameters() if param.requires_grad], lr=hparams[f'{pe}_{nn}']['lr'], weight_decay=hparams[f'{pe}_{nn}']['wd'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.87)\n",
    "\n",
    "model, optimizer, dataloader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, dataloader, scheduler\n",
    ")\n",
    "\n",
    "eval_dataloader = None\n",
    "earlystopper = None\n",
    "for epoch in range(10):\n",
    "    train_1epoch(dataloader, eval_dataloader, earlystopper, model, model.vision_processor, model.text_processor, optimizer, scheduler, device, accelerator)\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    torch.save(unwrapped_model, 'checkpoints/g3_{}_.pth'.format(epoch))\n",
    "\n",
    "    with open(\"threshold_accuracy.txt\", \"a\") as f:\n",
    "        f.write('checkpoints/g3_{}_.pth\\n'.format(epoch))\n",
    "\n",
    "\n",
    "    predictor = ZeroShotPredictor(model='checkpoints/g3_{}_.pth'.format(epoch), device=device)\n",
    "    df, res = predictor.evaluate_im2gps3k(\n",
    "        df_path=\"data/im2gps3k/im2gps3k_places365.csv\",\n",
    "        top_k=5,\n",
    "        root_path=\"data/im2gps3k/\",\n",
    "        image_data_path=\"filtered_im2gps3k\",\n",
    "        text_data_path= \"im2gps3k_places365.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv checkpoints/g3_5_.pth checkpoints/sirensh_6epoch_tmp.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_file\n",
    "\n",
    "upload_file(\n",
    "    path_or_fileobj=\"checkpoints/sirensh_6epoch_tmph.pth\",        # Local file path\n",
    "    path_in_repo=\"sirensh_6epoch_tmp.pth\",                         # Desired path in the repo\n",
    "    repo_id=\"tduongvn/Checkpoints-ACMMM25\",               # e.g., \"tungduong/my-model-repo\"\n",
    "    repo_type=\"model\",                                    # or \"dataset\" or \"space\"\n",
    "    commit_message=\"Upload sirensh_6epoch_tmp.pth\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff46c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
